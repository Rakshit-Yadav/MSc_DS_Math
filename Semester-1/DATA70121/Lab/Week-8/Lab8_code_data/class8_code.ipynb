{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries needed\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "# load machine learning libraries\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "# load plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Seed random number generator\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "def construct_polynomial_features(X, d):\n",
    "    \"\"\"\n",
    "    Constructs polynomial features of a given degree for the input data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : the input data with shape (N,).\n",
    "    d : the degree of the polynomial features to transform X into.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    poly_features : The polynomial features [X^0, X^1, ..., X^d] with shape (N, d+1).\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1, 1) # transform the shape (N,) into shape (N, 1)\n",
    "    poly_transformer = PolynomialFeatures(degree=d)  # Create PolynomialFeatures instance with the specified degree\n",
    "    poly_features = poly_transformer.fit_transform(X)  # Transform the input data into polynomial features\n",
    "    return poly_features\n",
    "\n",
    "\n",
    "def predict_with_poly_model(X, poly_model):\n",
    "    \"\"\"\n",
    "    Predicts the target value for given input data X using the provided polynomial model.\n",
    "\n",
    "    Parameters:\n",
    "    X: input data of shape (N,).\n",
    "    poly_model: a polynomial model.\n",
    "\n",
    "    Returns:\n",
    "    Predicted target values for the given input data of shape (N,)\n",
    "    \"\"\"\n",
    "    # Determine the degree of the polynomial from the shape of the model's coefficients\n",
    "    poly_degree = poly_model.coef_.shape[-1] - 1 \n",
    "\n",
    "    # Construct polynomial features for the input data\n",
    "    poly_features = construct_polynomial_features(X, poly_degree)\n",
    "\n",
    "    # Use the model to predict the target value for the input data\n",
    "    return poly_model.predict(poly_features)\n",
    "\n",
    "\n",
    "def fit_polynomial( X, y, d ):\n",
    "    \"\"\"\n",
    "    Fits an d-degree polynomial to the data (X, y) and returns the corresponding model.\n",
    "\n",
    "    Parameters:\n",
    "    X: input data of shape (N,)\n",
    "    y: target values of shape (N,)\n",
    "    d: degree of the polynomial to be fit\n",
    "\n",
    "    Returns:\n",
    "    model: a LinearRegression model fitted with polynomial features of degree d\n",
    "    \"\"\"\n",
    "    # Create a Linear Regression model without an intercept, \n",
    "    # as it's already included in the polynomial features\n",
    "    model = LinearRegression(fit_intercept=False) \n",
    "    \n",
    "    # Construct polynomial features\n",
    "    poly_features = construct_polynomial_features(X, d)\n",
    "    \n",
    "    # Fit the model with the polynomial features\n",
    "    model.fit(poly_features, y)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Demonstration of polynomial models fit to example datapoints\n",
    "# You should experiment with different model degrees and different \n",
    "# numbers of sampled datapoints to get a feel for how the degree of\n",
    "# the model affects the 'fit' of the model.\n",
    "\n",
    "# Define a ground truth linear model\n",
    "true_model = LinearRegression() # Initialize a LinearRegression object\n",
    "true_model.coef_ = np.array([1., -4., 1.]) # Set coefficients for a 2-degree polynomial linear model\n",
    "true_model.intercept_ = 0.  # Set intercept to 0 as it's already included in the polynomial features\n",
    "\n",
    "# Generate sample data using the defined linear model with added noise\n",
    "eps_std = 1.0 # Set standard deviation of noise\n",
    "num_samples = 15 # Define the number of data samples to generate\n",
    "\n",
    "X = np.random.uniform(-4., 4., size=num_samples) # Generate random sample points within the specified range\n",
    "eps = np.random.normal(scale=eps_std, size=num_samples) # Generate random noise\n",
    "y = predict_with_poly_model(X, true_model) + eps # Generate target values with added noises.\n",
    "\n",
    "# Fit a polynomial model to the generated data\n",
    "poly_deg = 5 # Set degree of the polynomial to fit\n",
    "fitted_model = fit_polynomial(X, y, poly_deg) # Fit the polynomial to the data\n",
    "\n",
    "# Plot the generated data\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X,y)\n",
    "plt.ylim(y.min()-1., y.max()+1.)\n",
    "\n",
    "# Generate sequence of x-values for plotting\n",
    "axis_X = np.arange(-4.0, 4.0, 0.2).reshape(-1, 1)\n",
    "\n",
    "# Plot the true polynomial function\n",
    "poly_y_true = predict_with_poly_model(axis_X, true_model) # Calculate true y-values\n",
    "plt.plot(axis_X, poly_y_true, label='True function')\n",
    "\n",
    "# Plot the fitted polynomial function\n",
    "poly_y_fitted = predict_with_poly_model(axis_X, fitted_model) # Calculate fitted y-values\n",
    "plt.plot(axis_X, poly_y_fitted, label='Fitted function')\n",
    "\n",
    "plt.title(\"Degree {:d} polynomial fit to example datapoints\".format(poly_deg))\n",
    "plt.legend() # Add legend to the plot\n",
    "\n",
    "plt.show() # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 1 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for calculating MSE, bias, and variance from fitted parameters\n",
    "def mean_square_error(pred_test_y_array, y):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) of the model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "    pred_test_y_array: An array of shape (num_resamplings, N) representing the predictions from \n",
    "                       different models on the test set.\n",
    "    y: An array of shape (N,) representing the target values for which we are calculating the MSE.\n",
    "\n",
    "    Returns:\n",
    "    The Mean Squared Error of the model's predictions.\n",
    "    \"\"\"\n",
    "    # Return the mean of the squared differences between the predictions and the true values\n",
    "    return np.mean(np.square(pred_test_y_array - np.expand_dims(y, 0)))\n",
    "\n",
    "def square_bias(pred_test_y_array, true_y):\n",
    "    \"\"\"\n",
    "    Calculate the squared bias of the model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "    pred_test_y_array: An array of shape (num_resamplings, N) representing the predictions from \n",
    "                       different models on the test set.\n",
    "    true_y: An array of shape (N,) representing the true target values in the test set.\n",
    "\n",
    "    Returns:\n",
    "    The squared bias of the model's predictions.\n",
    "    \"\"\"\n",
    "    # Compute the mean prediction across all models\n",
    "    mean_pred_y = np.mean(pred_test_y_array, axis=0)\n",
    "    \n",
    "    # Return the mean of the squared difference between the mean prediction and the true values\n",
    "    return np.mean(np.square(mean_pred_y - true_y))\n",
    "\n",
    "def variance(pred_test_y_array):\n",
    "    \"\"\"\n",
    "    Calculate the variance of the model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "    pred_test_y_array: An array of shape (num_resamplings, N) representing the predictions from \n",
    "                       different models on the test set.\n",
    "\n",
    "    Returns:\n",
    "    The variance of the model's predictions.\n",
    "    \"\"\"\n",
    "    # Return the mean of the variances of the predictions across all models\n",
    "    return np.mean(np.var(pred_test_y_array, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we sample the training dataset from the ground truth model with a fixed sample size(15).\n",
    "num_resamplings = 5 \n",
    "\n",
    "# Load the saved dataset\n",
    "X, y, test_X, test_y, true_test_y = np.load(\"./class8_generated_data.npy\", allow_pickle=True)\n",
    "\n",
    "# Degrees of polynomials to be considered\n",
    "polynomial_degrees = [2, 4, 6, 8]\n",
    "\n",
    "# Prepare a grid of subplots for visualizing the fitted polynomials and their performance metrics.\n",
    "fig, axes = plt.subplots(nrows=num_resamplings+1,\n",
    "                             ncols=len(polynomial_degrees),\n",
    "                             figsize=(12, 12), )\n",
    "\n",
    "# Divide the entire data into num_resamplings subsets (each of 15 samples)\n",
    "subsets_X = np.split(X, num_resamplings)\n",
    "subsets_y = np.split(y, num_resamplings)\n",
    "\n",
    "# Generate a range of x-values for plotting the fitted polynomials\n",
    "axis_X = np.arange(-3.0, 3.0, 0.2)\n",
    "\n",
    "# Iterate over each degree in the list of polynomial degrees   \n",
    "for j, degree in enumerate(polynomial_degrees):\n",
    "    \n",
    "    # Initialize a list to store predictions on the test set from multiple models\n",
    "    pred_test_y_multiple_models = []\n",
    "    \n",
    "    for i in range(num_resamplings):\n",
    "        # Extract the ith subset of data\n",
    "        subset_X = subsets_X[i] # array of shape (N_train,)\n",
    "        subset_y = subsets_y[i] # array of shape (N_train,)\n",
    "        \n",
    "        # Plot the data points in the ith subset\n",
    "        ax = axes[i, j] # select the corresponding axis\n",
    "        ax.set_ylim([y.min()-1., y.max()+1.])\n",
    "        ax.scatter(subset_X, subset_y)\n",
    "          \n",
    "        # Train a polynomial regression model on the ith subset\n",
    "        fitted_model = fit_polynomial(subset_X, subset_y, degree)\n",
    "\n",
    "        # Generate the y-values predicted by the model over the range of x-values\n",
    "        pred_y = predict_with_poly_model(axis_X, fitted_model) # array of shape (N_train,)\n",
    "\n",
    "        # Plot the fitted polynomial\n",
    "        ax.plot(axis_X, pred_y)\n",
    "        \n",
    "        # Obtain the model's predictions on the test set\n",
    "        pred_test_y = predict_with_poly_model(test_X, fitted_model) # array of shape (N_test,)\n",
    "        \n",
    "        # Store these predictions for later analysis\n",
    "        pred_test_y_multiple_models.append(pred_test_y)\n",
    "    \n",
    "    # Label the top subplots with the degree of the polynomial\n",
    "    axes[0, j].set_title(f\"Degree {degree}\")\n",
    "\n",
    "    \n",
    "    # convert the list into a numpy array of shape (num_resamplings, N_test)\n",
    "    pred_test_y_array = np.array(pred_test_y_multiple_models) \n",
    "\n",
    "    # compute and plot the MSE, squared bias, and variance\n",
    "    mse = mean_square_error(pred_test_y_array, test_y)\n",
    "    bias = square_bias(pred_test_y_array, true_test_y)\n",
    "    var = variance(pred_test_y_array)\n",
    "    \n",
    "    # Display these errors as a bar graph in the bottom subplot\n",
    "    labels = ['Total Error', 'Bias^2', 'Variance']\n",
    "    positions = [-2, 0, 2]\n",
    "    axes[-1, j].bar(positions, [mse, bias, var])\n",
    "    axes[-1, j].xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "    axes[-1, j].xaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "polynomial_degrees = [1, 3]\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1** From the plots, describe (i) how the bias and variance relate to a degree of the polynomial and (ii)\n",
    "which polynomial degree has been used to generate the data set.\n",
    "\n",
    "<span style=\"color:blue\">NOTE: The y-axis scales of the bar graphs are different.</span>\n",
    "\n",
    "**Answer**: ??? (This part is to be completed by the student. Please provide your analysis and justification here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 1 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignments 2 and 3 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ftse_prices = []\n",
    "with open('./class8_data_FTSE100.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    _ = next(csvreader) # skip the header row\n",
    "    for row in csvreader:\n",
    "        ftse_prices.append(float(row[1].replace(',','')) / 1e3)\n",
    "ftse_prices.reverse()\n",
    "y = np.array(ftse_prices) # 1e-3 * price of ftse\n",
    "X = np.arange(np.size(y)) # number of months since start\n",
    "\n",
    "\n",
    "# Define the polynomial degrees to consider for model fitting\n",
    "polynomial_degrees = [1, 3]\n",
    "\n",
    "# Initialize a figure with two subplots (one for each degree) in a 1x2 grid and size 10x5\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Loop over each polynomial degree\n",
    "for i, degree in enumerate(polynomial_degrees):\n",
    "    # Select the corresponding axis for this degree\n",
    "    ax = axes[i] \n",
    "\n",
    "    # Plot the original data as scatter plot on the current axis\n",
    "    ax.scatter(X, y)\n",
    "    # Set the y-axis limits to be slightly larger than the data range for better visualization\n",
    "    ax.set_ylim(y.min()-1., y.max()+1.)\n",
    "\n",
    "    # Fit a polynomial model of the current degree to the entire training data set\n",
    "    full_dataset_model = fit_polynomial(X, y, degree)\n",
    "    # Predict y-values over the entire x-range using the fitted model\n",
    "    full_dataset_poly_y = predict_with_poly_model(X, full_dataset_model)\n",
    "    \n",
    "    # Plot the fitted model prediction on the current axis\n",
    "    ax.plot(X, full_dataset_poly_y, label=f\"Polynomial Degree {degree}\")\n",
    "    \n",
    "    # Label the y-axis as Price in thousands of pounds\n",
    "    ax.set_ylabel(\"Price (Â£x1000)\")\n",
    "    # Label the x-axis as the number of months since September 2008\n",
    "    ax.set_xlabel(\"Months since September 2008\")\n",
    "    # Set the title of the subplot to indicate the price and the degree of the polynomial\n",
    "    ax.set_title(f\"FTSE 100 index price (Degree {degree})\")\n",
    "    # Display a legend on the current axis\n",
    "    ax.legend()\n",
    "\n",
    "# Ensure the subplots do not overlap and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform K-fold cross validation\n",
    "def Kfold_cross_validation(X, y, degree, num_folds, verbose=False):\n",
    "    \"\"\"\n",
    "    This function is used to perform K-fold Cross Validation. \n",
    "    It trains a polynomial model on training data and calculates the Mean Squared Error (MSE) on validation data.\n",
    "    \n",
    "    Parameters:\n",
    "    X: The input dataset.\n",
    "    y: The target values.\n",
    "    degree: The degree of the polynomial we want to fit.\n",
    "    num_folds: The number of folds for cross validation.\n",
    "    verbose: If True, prints MSE for each fold.\n",
    "\n",
    "    TODO: You are required to calculate the mean and standard deviation of MSE across all folds and print them.\n",
    "    \"\"\"\n",
    "    # Record the start time of cross validation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize the KFold object\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    # Create an empty list to store mean squared error (MSE) for each fold\n",
    "    mse_losses = []\n",
    "    \n",
    "    # For each fold, train the model and calculate the MSE\n",
    "    for i, (train_index, validation_index) in enumerate(kf.split(X)):\n",
    "        # Extract training and validation data for the current fold\n",
    "        train_X, validation_X = X[train_index], X[validation_index]\n",
    "        train_y, validation_y = y[train_index], y[validation_index]\n",
    "\n",
    "        # Fit the polynomial model to the training data\n",
    "        fitted_model = fit_polynomial(train_X, train_y, degree)\n",
    "        \n",
    "        # Use the fitted model to make predictions on the validation data\n",
    "        pred_y = predict_with_poly_model(validation_X, fitted_model)\n",
    "        \n",
    "        # Calculate MSE for the current fold and store it\n",
    "        fold_mean_squared_error = np.mean(np.square(pred_y - validation_y))\n",
    "        mse_losses.append(fold_mean_squared_error)\n",
    "        \n",
    "        # IMPORTANT: For Leave-One-Out Cross Validation (LOOCV), set 'verbose' to False. \n",
    "        # This prevents excessive output, as LOOCV produces an output for each data point.\n",
    "        if verbose:\n",
    "            print(\"Mean loss for fold {:d}/{:d}: {:.5f}\".format(i+1, num_folds, fold_mean_squared_error))\n",
    "    \n",
    "    end_time = time.time() # Record the end time of cross validation\n",
    "    elapsed_t_ms = (end_time - start_time) * 1e3\n",
    "    print(\"Completed {:d} folds in {:.2f} milliseconds. \\n\".format(num_folds, elapsed_t_ms))\n",
    "    \n",
    "    # TODO: Calculate the mean and standard deviation of MSE across all folds and print them\n",
    "    # Hint: You can use np.mean and np.std functions\n",
    "    # IMPORTANT: Replace the placeholder values (-1000) with the right values.\n",
    "    mean_mse_loss = -1000\n",
    "    std_mse_loss = -1000\n",
    "    print(\"Mean: {:.3f}, std: {:.3f}\".format(mean_mse_loss, std_mse_loss))\n",
    "\n",
    "\n",
    "# Function to perform Leave-One-Out Cross Validation (LOOCV)\n",
    "def LOO_cross_validation(X, y, degree):\n",
    "    \"\"\"\n",
    "    This function is used to perform Leave-One-Out Cross Validation (LOOCV). \n",
    "    \n",
    "    LOOCV is a particular case of k-fold cross-validation where the number of folds equals the number of\n",
    "    data points.\n",
    "\n",
    "    Parameters:\n",
    "    X: The input dataset.\n",
    "    y: The target values.\n",
    "    degree: The degree of the polynomial we want to fit.\n",
    "\n",
    "    TODO: This function is incomplete. You are required to complement the rest of LOOCV.\n",
    "          You can use the Kfold_cross_validation function as a reference.  \n",
    "    \"\"\"\n",
    "    num_data = np.shape(X)[0]\n",
    "    \n",
    "    # TODO: Complement the rest of LOOCV.\n",
    "    print(\"LOOCV - Incomplete\")\n",
    "    \n",
    "    \n",
    "# Perform cross validation with different polynomial degrees in the list 'polynomial_degrees'.\n",
    "for degree in polynomial_degrees:\n",
    "    print(\"Running 5-fold cross validation on polynomial model of degree {:d}\\n\".format(degree))\n",
    "    Kfold_cross_validation(X, y, degree, 5, verbose=True)\n",
    "    \n",
    "    print(f\"\\n{'-' * 50}\\n\") # separator\n",
    "    \n",
    "    print(\"Running LOOCV on polynomial model of degree {:d}\\n\".format(degree))\n",
    "    LOO_cross_validation(X, y, degree)\n",
    "    \n",
    "    print(f\"\\n{'-' * 50}\\n\") # separator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2** Complement the LOOCV code. Based on the experimental results, comment on the difference between the two cross-validation methods in terms of computational efficiency.\n",
    "\n",
    "**Answer**: ??? (This part is to be completed by yourself. Please provide your code, results and analysis.)\n",
    "\n",
    "**Assignment 3** Calculate the average, and standard-deviation, of the mean square error loss on the validation set using LOOCV and record your results.\n",
    "\n",
    "**Answer**: ??? (This part is to be completed based on running the LOOCV code implemented by yourself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignments 2 and 3 ===========</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
